{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "## Machine Translation\n",
    "In this notebook, we'll build a deep neural network that functions as part of an end-to-end machine translation pipeline. Our completed pipeline will accept English text as input and return the French translation as output. This project was taken from the [Udacity AI Nanodegree](https://www.udacity.com/course/artificial-intelligence-nanodegree--nd889). We can break this project down into three steps:\n",
    "\n",
    "1. **Preprocess** - We'll need to convert text to a sequence of integers.\n",
    "2. **Modeling** - Then, we'll create models which accepts a sequence of integers as input and returns a probability distribution over possible translations.\n",
    "3. **Prediction** - Lastly, we'll write a function to run our model on English text and see our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "The most common datasets used for machine translation are from [WMT](http://www.statmt.org/). However, that will take a long time to train a neural network on.  Instead, we'll be using a dataset that contains a small vocabulary which can be found in the `data/` folder. The `small_vocab_en` file contains English sentences with their French translations in the `small_vocab_fr` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Loaded\n",
      "small_vocab_en Line 1:  new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "small_vocab_fr Line 1:  new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n",
      "small_vocab_en Line 2:  the united states is usually chilly during july , and it is usually freezing in november .\n",
      "small_vocab_fr Line 2:  les Ã©tats-unis est gÃ©nÃ©ralement froid en juillet , et il gÃ¨le habituellement en novembre .\n",
      "small_vocab_en Line 3:  california is usually quiet during march , and it is usually hot in june .\n",
      "small_vocab_fr Line 3:  california est gÃ©nÃ©ralement calme en mars , et il est gÃ©nÃ©ralement chaud en juin .\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Load dataset from a given path.\n",
    "    \"\"\"\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\") as f:\n",
    "        data = f.read()\n",
    "\n",
    "    return data.split('\\n')\n",
    "\n",
    "\n",
    "# Load English data\n",
    "english_sentences = load_data('data/small_vocab_en')\n",
    "# Load French data\n",
    "french_sentences = load_data('data/small_vocab_fr')\n",
    "print('Dataset Loaded')\n",
    "\n",
    "# Sample from dataset\n",
    "for sample_i in range(3):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the complexity of the problem. A more complex vocabulary is a more complex problem. Let's look at the complexity of the dataset we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1823250 English words.\n",
      "227 unique English words.\n",
      "10 Most common words in the English dataset:\n",
      "\"is\" \",\" \".\" \"in\" \"it\" \"during\" \"the\" \"but\" \"and\" \"sometimes\"\n",
      "\n",
      "1961295 French words.\n",
      "355 unique French words.\n",
      "10 Most common words in the French dataset:\n",
      "\"est\" \".\" \",\" \"en\" \"il\" \"les\" \"mais\" \"et\" \"la\" \"parfois\"\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "\n",
    "# Create a counter object for each dataset\n",
    "english_words_counter = collections.Counter([word for sentence in english_sentences for word in sentence.split()])\n",
    "french_words_counter = collections.Counter([word for sentence in french_sentences for word in sentence.split()])\n",
    "\n",
    "print('{} English words.'.format(len([word for sentence in english_sentences for word in sentence.split()])))\n",
    "print('{} unique English words.'.format(len(english_words_counter)))\n",
    "print('10 Most common words in the English dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*english_words_counter.most_common(10)))[0]) + '\"')\n",
    "print()\n",
    "print('{} French words.'.format(len([word for sentence in french_sentences for word in sentence.split()])))\n",
    "print('{} unique French words.'.format(len(french_words_counter)))\n",
    "print('10 Most common words in the French dataset:')\n",
    "print('\"' + '\" \"'.join(list(zip(*french_words_counter.most_common(10)))[0]) + '\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are using an abridged dataset, we'll only need to learn about 355 unique French words. For comparison, _Alice's Adventures in Wonderland_ contains 2,766 unique words of a total of 15,500 words. If we wanted to learn larger vocabularies it would just require more data and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Normally, the text we are given will be messy. For example, if we scrape a website for our vocabulary, we'll end up with a bunch of HTML tags and markup that aren't useful inputs for our objective. For this reason, text processing is usually our first step in Natural Language Processing. Common text processing steps include:\n",
    "* Cleaning - removing unwanted symbols, tags, stopwords, etc so that we are left with plain text.\n",
    "* Normalization - making everything lowercase, removing punctuation, etc.\n",
    "* Tokenization - converting words to symbols that can fed into our model.\n",
    "\n",
    "From looking at the sentences above, we can see they have mostly been preprocessed already. The punctuations have been delimited using spaces. All the text has been converted to lowercase. This will save us some time. We still need to tokenize our data though. We'll convert the text into sequences of integers using the following preprocess methods:\n",
    "1. Tokenize the words into ids.\n",
    "2. Add padding to make all the sequences the same length.\n",
    "3. Run our data through both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "For a neural network to predict on text data, it first has to be turned into data it can understand. Text data like \"dog\" is a sequence of ASCII character encodings. Since a neural network is a series of multiplication and addition operations, the input data needs to be numbers.\n",
    "\n",
    "We can turn each character into a number or each word into a number. These are called character and word ids, respectively. Character ids are used for character level models that generate text predictions for each character.  A word level model uses word ids that generate text predictions for each word. Word level models tend to learn better, since they are lower in complexity, so we'll use those.\n",
    "\n",
    "We'll turn each sentence into a sequence of words ids using Keras's [`Tokenizer`](https://keras.io/preprocessing/text/#tokenizer) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 1, 'quick': 2, 'a': 3, 'brown': 4, 'fox': 5, 'jumps': 6, 'over': 7, 'lazy': 8, 'dog': 9, 'by': 10, 'jove': 11, 'my': 12, 'study': 13, 'of': 14, 'lexicography': 15, 'won': 16, 'prize': 17, 'this': 18, 'is': 19, 'short': 20, 'sentence': 21}\n",
      "\n",
      "Sequence 1 in x\n",
      "  Input:  The quick brown fox jumps over the lazy dog .\n",
      "  Output: [1, 2, 4, 5, 6, 7, 1, 8, 9]\n",
      "Sequence 2 in x\n",
      "  Input:  By Jove , my quick study of lexicography won a prize .\n",
      "  Output: [10, 11, 12, 2, 13, 14, 15, 16, 3, 17]\n",
      "Sequence 3 in x\n",
      "  Input:  This is a short sentence .\n",
      "  Output: [18, 19, 3, 20, 21]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    # Initate tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "    \n",
    "    # Fit tokenizer to text\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    \n",
    "    # Get tokenized data\n",
    "    tokenized_data = tokenizer.texts_to_sequences(x)\n",
    "    \n",
    "    return tokenized_data, tokenizer\n",
    "\n",
    "\n",
    "# Tokenize Example output\n",
    "text_sentences = ['The quick brown fox jumps over the lazy dog .',\n",
    "                  'By Jove , my quick study of lexicography won a prize .',\n",
    "                  'This is a short sentence .']\n",
    "text_tokenized, text_tokenizer = tokenize(text_sentences)\n",
    "print(text_tokenizer.word_index)\n",
    "print()\n",
    "for sample_i, (sent, token_sent) in enumerate(zip(text_sentences, text_tokenized)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(sent))\n",
    "    print('  Output: {}'.format(token_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding\n",
    "\n",
    "When batching the sequence of word ids together, each sequence needs to be the same length. Since sentences are dynamic in length, we can add padding to the end of the sequences to make them the same length. We'll be using Keras's [`pad_sequences`](https://keras.io/preprocessing/sequence/#pad_sequences) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 1 in x\n",
      "  Input:  [1 2 4 5 6 7 1 8 9]\n",
      "  Output: [1 2 4 5 6 7 1 8 9 0]\n",
      "Sequence 2 in x\n",
      "  Input:  [10 11 12  2 13 14 15 16  3 17]\n",
      "  Output: [10 11 12  2 13 14 15 16  3 17]\n",
      "Sequence 3 in x\n",
      "  Input:  [18 19  3 20 21]\n",
      "  Output: [18 19  3 20 21  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # Initate base case as length of longest sequence\n",
    "    if length is None:\n",
    "        length = max(len(seq) for seq in x)\n",
    "        \n",
    "    # Get padded sequences\n",
    "    padded_seq = pad_sequences(sequences=x, maxlen=length, padding='post')\n",
    "    \n",
    "    return padded_seq\n",
    "\n",
    "\n",
    "# Pad Tokenized output\n",
    "test_pad = pad(text_tokenized)\n",
    "for sample_i, (token_sent, pad_sent) in enumerate(zip(text_tokenized, test_pad)):\n",
    "    print('Sequence {} in x'.format(sample_i + 1))\n",
    "    print('  Input:  {}'.format(np.array(token_sent)))\n",
    "    print('  Output: {}'.format(pad_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the Data\n",
    "Let's run our data through our tokenizer and pad it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preprocessed\n",
      "small_vocab_en Line 1:  [17 23  1  8 67  4 39  7  3  1 55  2 44  0  0]\n",
      "small_vocab_fr Line 1:  [[ 35]\n",
      " [ 34]\n",
      " [  1]\n",
      " [  8]\n",
      " [ 67]\n",
      " [ 37]\n",
      " [ 11]\n",
      " [ 24]\n",
      " [  6]\n",
      " [  3]\n",
      " [  1]\n",
      " [112]\n",
      " [  2]\n",
      " [ 50]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]]\n",
      "small_vocab_en Line 2:  [ 5 20 21  1  9 62  4 43  7  3  1  9 51  2 45]\n",
      "small_vocab_fr Line 2:  [[ 4]\n",
      " [32]\n",
      " [31]\n",
      " [ 1]\n",
      " [12]\n",
      " [19]\n",
      " [ 2]\n",
      " [49]\n",
      " [ 6]\n",
      " [ 3]\n",
      " [95]\n",
      " [69]\n",
      " [ 2]\n",
      " [51]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]\n",
      " [ 0]]\n",
      "small_vocab_en Line 3:  [22  1  9 67  4 38  7  3  1  9 68  2 34  0  0]\n",
      "small_vocab_fr Line 3:  [[101]\n",
      " [  1]\n",
      " [ 12]\n",
      " [ 67]\n",
      " [  2]\n",
      " [ 45]\n",
      " [  6]\n",
      " [  3]\n",
      " [  1]\n",
      " [ 12]\n",
      " [ 21]\n",
      " [  2]\n",
      " [ 41]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]\n",
      " [  0]]\n"
     ]
    }
   ],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    # Tokenize our data\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "    \n",
    "    # Pad our data\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "    preprocess_y = pad(preprocess_y)\n",
    "\n",
    "    # Reshape our data\n",
    "    # Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "    preprocess_y = preprocess_y.reshape(*preprocess_y.shape, 1)\n",
    "\n",
    "    return preprocess_x, preprocess_y, x_tk, y_tk\n",
    "\n",
    "preproc_english_sentences, preproc_french_sentences, english_tokenizer, french_tokenizer =\\\n",
    "    preprocess(english_sentences, french_sentences)\n",
    "\n",
    "print('Data Preprocessed')\n",
    "# Sample from processed dataset\n",
    "for sample_i in range(3):\n",
    "    print('small_vocab_en Line {}:  {}'.format(sample_i + 1, preproc_english_sentences[sample_i]))\n",
    "    print('small_vocab_fr Line {}:  {}'.format(sample_i + 1, preproc_french_sentences[sample_i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ids Back to Text\n",
    "The neural network will be translating the input to words ids, which isn't the final form we want. We want the French translation. Let's write a function, `logits_to_text`, that will bridge the gap between the logits from the neural network to the French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`logits_to_text` function loaded.\n"
     ]
    }
   ],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    # Extract word from our tokenizer\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    # Insert '<PAD>' in place of zeros.\n",
    "    index_to_words[0] = '<PAD>'\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print('`logits_to_text` function loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "Now that our data is processed, let's build our models. We'll start by experimenting with various neural network architectures:\n",
    "* Simple RNN\n",
    "* RNN with Embedding\n",
    "* Bidirectional RNN\n",
    "* Encoder-Decoder RNN\n",
    "\n",
    "After experimenting with the four simple architectures, we will construct a deeper architecture that is designed to outperform all four models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Simple RNN\n",
    "![RNN](images/rnn.png)\n",
    "A basic RNN model is a good baseline for sequence data. Unlike other neural networks, RNNs are able to remember the previous state of a neural network and use it as input to the next calculation. This allows RNNs to learn patterns in sequences, such as the next word in a sentence based on the first few words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 53s - loss: 1.3977 - acc: 0.6218 - val_loss: nan - val_acc: 0.6770\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 51s - loss: 0.8787 - acc: 0.7146 - val_loss: nan - val_acc: 0.7555\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 50s - loss: 0.7252 - acc: 0.7578 - val_loss: nan - val_acc: 0.7849\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 51s - loss: 0.6244 - acc: 0.7910 - val_loss: nan - val_acc: 0.7836\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 47s - loss: 0.5646 - acc: 0.8101 - val_loss: nan - val_acc: 0.8329\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 50s - loss: 0.5327 - acc: 0.8192 - val_loss: nan - val_acc: 0.8305\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 50s - loss: 0.5073 - acc: 0.8263 - val_loss: nan - val_acc: 0.8223\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 50s - loss: 0.4571 - acc: 0.8420 - val_loss: nan - val_acc: 0.8563\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 51s - loss: 0.4665 - acc: 0.8384 - val_loss: nan - val_acc: 0.8390\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 51s - loss: 0.4241 - acc: 0.8514 - val_loss: nan - val_acc: 0.8705\n",
      "Model predicts:\n",
      "new jersey est parfois calme en l' automne l' il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Actual translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import GRU, Input, Dense, TimeDistributed\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "\n",
    "\n",
    "def simple_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a basic RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Initate model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Gated Recurrent Layers\n",
    "    # Return sequences set to True to remember full sequence and not just last output\n",
    "    # Add modest recurrent dropout to prevent overfitting\n",
    "    model.add(GRU(1024, input_shape=input_shape[1:], return_sequences=True))\n",
    "    model.add(GRU(512, return_sequences=True, recurrent_dropout=0.3))\n",
    "    \n",
    "    # Add fully connected layer and softmax activation\n",
    "    # Add a Time distributed wrapper to dense layer\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Compile\n",
    "    learning_rate = .001\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Reshape the input to work with a simple RNN\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "simple_rnn_model = simple_model(tmp_x.shape,\n",
    "                                preproc_french_sentences.shape[1],\n",
    "                                len(english_tokenizer.word_index),\n",
    "                                len(french_tokenizer.word_index))\n",
    "\n",
    "simple_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=256, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print predictions\n",
    "print('Model predicts:')\n",
    "print(logits_to_text(simple_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "print('Actual translation:')\n",
    "print(french_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this simple RNN we achieved a valuation accuracy of about 87%.\n",
    "* Prediction: new jersey est parfois calme en l' automne l' il est neigeux en avril\n",
    "* Actual: new jersey est parfois calme pendant l' automne et il est neigeux en avril"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Embedding\n",
    "![RNN](images/embedding.png)\n",
    "We've turned our words into ids, but there's a better representation of a word called word embeddings. An embedding is a vector representation of the word that is close to similar words in n-dimensional space, where the n represents the size of the embedding vectors.\n",
    "\n",
    "In other words, we take our words and run them through a seperate neural network that outputs how ever many features we want. Words that are similar in meaning will be closer together. We'll be using Kera's [`embedding`](https://keras.io/layers/embeddings/) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 61s - loss: 0.8299 - acc: 0.7928 - val_loss: nan - val_acc: 0.9006\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 59s - loss: 0.2611 - acc: 0.9115 - val_loss: nan - val_acc: 0.9226\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 59s - loss: 0.2120 - acc: 0.9258 - val_loss: nan - val_acc: 0.9316\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 59s - loss: 0.1916 - acc: 0.9320 - val_loss: nan - val_acc: 0.9334\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 59s - loss: 0.1806 - acc: 0.9347 - val_loss: nan - val_acc: 0.9342\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 59s - loss: 0.1731 - acc: 0.9370 - val_loss: nan - val_acc: 0.9358\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 59s - loss: 0.1681 - acc: 0.9383 - val_loss: nan - val_acc: 0.9362\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 59s - loss: 0.1647 - acc: 0.9391 - val_loss: nan - val_acc: 0.9359\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 59s - loss: 0.1622 - acc: 0.9399 - val_loss: nan - val_acc: 0.9378\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 59s - loss: 0.1571 - acc: 0.9411 - val_loss: nan - val_acc: 0.9376\n",
      "Model predicts:\n",
      "new jersey est parfois calme en l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Actual translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a RNN model using word embedding on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Initate model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add embedding layer\n",
    "    model.add(Embedding(english_vocab_size, 1024, input_length=input_shape[1]))\n",
    "    \n",
    "    # Add Gated Recurrent Layers\n",
    "    # Return sequences set to True to remember full sequence and not just last output\n",
    "    # Add modest recurrent dropout to prevent overfitting\n",
    "    model.add(GRU(1024, return_sequences=True))\n",
    "    model.add(GRU(512, return_sequences=True, recurrent_dropout=0.3))\n",
    "    \n",
    "    # Add fully connected layer and softmax activation\n",
    "    # Add a Time distributed wrapper to dense layer\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Compile\n",
    "    learning_rate = .001\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Reshape the input to work with embeddings\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "\n",
    "# Train the neural network\n",
    "embed_rnn_model = embed_model(tmp_x.shape,\n",
    "                              preproc_french_sentences.shape[1],\n",
    "                              len(english_tokenizer.word_index),\n",
    "                              len(french_tokenizer.word_index))\n",
    "\n",
    "embed_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=256, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction\n",
    "print('Model predicts:')\n",
    "print(logits_to_text(embed_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "print('Actual translation:')\n",
    "print(french_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this Embedded RNN we achieved a valuation accuracy of about 93%.\n",
    "* Prediction: new jersey est parfois calme en l' automne et il est neigeux en avril\n",
    "* Actual: new jersey est parfois calme pendant l' automne et il est neigeux en avril"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Bidirectional RNNs\n",
    "![RNN](images/bidirectional.png)\n",
    "One limitation of a RNN is that it can't see the future sequence input, only the past. However, Bidirectional RNNs allow the network to read future input information from its current state. This allows us to find context information not only from the words preceding our target, but also from the words following it. We'll be useing Kera's [`bidirectional`](https://keras.io/layers/wrappers/#bidirectional) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 112s - loss: 1.2240 - acc: 0.6520 - val_loss: nan - val_acc: 0.7052\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 109s - loss: 0.7713 - acc: 0.7330 - val_loss: nan - val_acc: 0.7256\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 109s - loss: 0.6371 - acc: 0.7656 - val_loss: nan - val_acc: 0.7550\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 109s - loss: 0.5512 - acc: 0.7998 - val_loss: nan - val_acc: 0.8223\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 109s - loss: 0.4876 - acc: 0.8270 - val_loss: nan - val_acc: 0.8460\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 109s - loss: 0.4559 - acc: 0.8351 - val_loss: nan - val_acc: 0.8309\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 109s - loss: 0.4172 - acc: 0.8496 - val_loss: nan - val_acc: 0.8618\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 109s - loss: 0.4497 - acc: 0.8411 - val_loss: nan - val_acc: 0.8313\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 109s - loss: 0.3540 - acc: 0.8724 - val_loss: nan - val_acc: 0.8840\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 109s - loss: 0.4046 - acc: 0.8566 - val_loss: nan - val_acc: 0.8740\n",
      "Model predicts:\n",
      "new jersey est parfois calme au cours automne il automne neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Actual translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "\n",
    "def bd_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a bidirectional RNN model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Initate model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Gated Recurrent Layers\n",
    "    # Add Bidirectional wrapper to learn from future input\n",
    "    # Return sequences set to True to remember full sequence and not just last output\n",
    "    # Add modest recurrent dropout to prevent overfitting\n",
    "    model.add(Bidirectional(GRU(1024, return_sequences=True), input_shape=input_shape[1:]))\n",
    "    model.add(Bidirectional(GRU(512, return_sequences=True, recurrent_dropout=0.3)))\n",
    "    \n",
    "    # Add fully connected layer and softmax activation\n",
    "    # Add a Time distributed wrapper to dense layer\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Compile\n",
    "    learning_rate = .001\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Reshape the input to work with bidirectional model\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the model\n",
    "bd_rnn_model = bd_model(tmp_x.shape,\n",
    "                        preproc_french_sentences.shape[1],\n",
    "                        len(english_tokenizer.word_index),\n",
    "                        len(french_tokenizer.word_index))\n",
    "\n",
    "bd_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=256, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print prediction(s)\n",
    "print('Model predicts:')\n",
    "print(logits_to_text(bd_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "print('Actual translation:')\n",
    "print(french_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this Bidirectional RNN we achieved a valuation accuracy of about 87%.\n",
    "* Prediction: new jersey est parfois calme au cours automne il automne neigeux en avril\n",
    "* Actual: new jersey est parfois calme pendant l' automne et il est neigeux en avril"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Encoder-Decoder\n",
    "Another useful model is the Encoder-Decoder. As the name suggests, this model is made up of an encoder and decoder. The encoder creates a matrix representation of the sentence. The decoder takes this matrix as input and predicts the translation as output. Think of this as a two step neural network. One network comes up with an encoding; the other comes up with the decoding. We'll be useing Kera's [`repeatvector`](https://faroit.github.io/keras-docs/2.0.6/layers/core/#repeatvector) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 52s - loss: 1.9232 - acc: 0.5430 - val_loss: nan - val_acc: 0.6159\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 51s - loss: 1.2769 - acc: 0.6361 - val_loss: nan - val_acc: 0.6645\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 50s - loss: 1.1113 - acc: 0.6687 - val_loss: nan - val_acc: 0.6892\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 51s - loss: 0.9760 - acc: 0.6923 - val_loss: nan - val_acc: 0.6896\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 50s - loss: 0.8821 - acc: 0.7130 - val_loss: nan - val_acc: 0.7395\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 49s - loss: 0.7741 - acc: 0.7379 - val_loss: nan - val_acc: 0.7559\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 49s - loss: 0.7024 - acc: 0.7576 - val_loss: nan - val_acc: 0.7915\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 49s - loss: 0.6216 - acc: 0.7828 - val_loss: nan - val_acc: 0.8001\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 51s - loss: 0.5777 - acc: 0.7964 - val_loss: nan - val_acc: 0.8064\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 49s - loss: 0.5288 - acc: 0.8116 - val_loss: nan - val_acc: 0.8275\n",
      "Model predicts:\n",
      "new jersey est parfois chaud au l' et il automne et en est en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Actual translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import RepeatVector\n",
    "\n",
    "\n",
    "def encdec_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train an encoder-decoder model on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Initate model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add Gated Recurrent Layers\n",
    "    model.add(GRU(1024, input_shape=input_shape[1:], return_sequences=False))\n",
    "    # Add Repeat vector to repeat the last output\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    # Add modest recurrent dropout to prevent overfitting\n",
    "    model.add(GRU(512, return_sequences=True, recurrent_dropout=0.3))\n",
    "    \n",
    "    # Add fully connected layer and softmax activation\n",
    "    # Add a Time distributed wrapper to dense layer\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Compile\n",
    "    learning_rate = .001\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Reshape the input to work with model\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "tmp_x = tmp_x.reshape((-1, preproc_french_sentences.shape[-2], 1))\n",
    "\n",
    "# Train the neural network\n",
    "encdec_rnn_model = encdec_model(tmp_x.shape,\n",
    "                                preproc_french_sentences.shape[1],\n",
    "                                len(english_tokenizer.word_index),\n",
    "                                len(french_tokenizer.word_index))\n",
    "\n",
    "encdec_rnn_model.fit(tmp_x, preproc_french_sentences, batch_size=256, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print predictions\n",
    "print('Model predicts:')\n",
    "print(logits_to_text(encdec_rnn_model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "print('Actual translation:')\n",
    "print(french_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this Encoder-Decoder RNN we achieved a valuation accuracy of about 82%.\n",
    "* Prediction: new jersey est parfois chaud au l' et il automne et en est en\n",
    "* Actual: new jersey est parfois calme pendant l' automne et il est neigeux en avril"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Putting it all together\n",
    "Let's combine all our models together into a super model for machine translation. Well include embeddings, bidirectionallity, and encoder-decoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 127s - loss: 1.3056 - acc: 0.6721 - val_loss: nan - val_acc: 0.8297\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 123s - loss: 0.3682 - acc: 0.8935 - val_loss: nan - val_acc: 0.9423\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 124s - loss: 0.1731 - acc: 0.9501 - val_loss: nan - val_acc: 0.9570\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 124s - loss: 0.1197 - acc: 0.9650 - val_loss: nan - val_acc: 0.9700\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 124s - loss: 0.0918 - acc: 0.9729 - val_loss: nan - val_acc: 0.9760\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 125s - loss: 0.0745 - acc: 0.9780 - val_loss: nan - val_acc: 0.9742\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 126s - loss: 0.0667 - acc: 0.9803 - val_loss: nan - val_acc: 0.9778\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 126s - loss: 0.0569 - acc: 0.9833 - val_loss: nan - val_acc: 0.9793\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 126s - loss: 0.0536 - acc: 0.9844 - val_loss: nan - val_acc: 0.9781\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 125s - loss: 0.0481 - acc: 0.9860 - val_loss: nan - val_acc: 0.9815\n",
      "Model predicts:\n",
      "new jersey est parfois calme pendant l' automne et il est neigeux en avril <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Actual translation:\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n"
     ]
    }
   ],
   "source": [
    "def model_final(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    \"\"\"\n",
    "    Build and train a model that incorporates embedding, encoder-decoder, and bidirectional RNN on x and y\n",
    "    :param input_shape: Tuple of input shape\n",
    "    :param output_sequence_length: Length of output sequence\n",
    "    :param english_vocab_size: Number of unique English words in the dataset\n",
    "    :param french_vocab_size: Number of unique French words in the dataset\n",
    "    :return: Keras model built, but not trained\n",
    "    \"\"\"\n",
    "    # Initate model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add embedding layer\n",
    "    model.add(Embedding(english_vocab_size, 1024, input_length=input_shape[1]))\n",
    "    \n",
    "    # Add Gated Recurrent Layers, Bidirectional, RepeatVector, dropout\n",
    "    model.add(Bidirectional(GRU(1024, return_sequences=False)))\n",
    "    model.add(RepeatVector(output_sequence_length))\n",
    "    model.add(Bidirectional(GRU(512, return_sequences=True, recurrent_dropout=0.3)))\n",
    "    \n",
    "    # Add fully connected layer and softmax activation\n",
    "    # Add a Time distributed wrapper to dense layer\n",
    "    model.add(TimeDistributed(Dense(french_vocab_size)))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    # Compile\n",
    "    learning_rate = .001\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Reshape the input to work with model\n",
    "tmp_x = pad(preproc_english_sentences, preproc_french_sentences.shape[1])\n",
    "\n",
    "# Train the neural network\n",
    "model = model_final(tmp_x.shape,\n",
    "                    preproc_french_sentences.shape[1],\n",
    "                    len(english_tokenizer.word_index),\n",
    "                    len(french_tokenizer.word_index))\n",
    "\n",
    "model.fit(tmp_x, preproc_french_sentences, batch_size=256, epochs=10, validation_split=0.2)\n",
    "\n",
    "# Print predictions\n",
    "print('Model predicts:')\n",
    "print(logits_to_text(model.predict(tmp_x[:1])[0], french_tokenizer))\n",
    "print('Actual translation:')\n",
    "print(french_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our final RNN we achieved a valuation accuracy of about 98%. We could do even better with longer training times.\n",
    "* Prediction: new jersey est parfois calme pendant l' automne et il est neigeux en avril\n",
    "* Actual: new jersey est parfois calme pendant l' automne et il est neigeux en avril"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation Pipeline\n",
    "Lastly, let's create a function that can take in an unprocessed English sentence and return the French translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he saw a old yellow truck\n",
      "Translation:\n",
      "il a vu un vieux camion jaune <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Actual:\n",
      "il a vu un vieux camion jaune\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence):\n",
    "    \"\"\"\n",
    "    Takes in an English sentence and returns the translated French sentence.\n",
    "    \"\"\"\n",
    "    # Pre-process sentence\n",
    "    sentence = [english_tokenizer.word_index[word] for word in sentence.split()]\n",
    "    sentence = pad([sentence], length=21)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = logits_to_text(model.predict(sentence)[0], french_tokenizer)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "print('he saw a old yellow truck')\n",
    "print('Translation:')\n",
    "print(translate('he saw a old yellow truck'))\n",
    "print('Actual:')\n",
    "print('il a vu un vieux camion jaune')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, this only works for English words in our trained vocabulary. For a more general purpose translator, we would need to train on more data."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:aind]",
   "language": "python",
   "name": "conda-env-aind-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
